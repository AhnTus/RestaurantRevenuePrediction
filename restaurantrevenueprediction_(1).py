# -*- coding: utf-8 -*-
"""RestaurantRevenuePrediction (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-aGLNOOzn6B6faHAoNC3hR7_asT3Z6cv
"""

#import thư viện
import numpy as np
import pandas as pd
from math import sqrt
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import RFE, f_regression
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split,KFold
from sklearn.metrics import mean_squared_error

"""# Data Overview

Id : Restaurant id

Open Date : opening date for a restaurant

City : City that the restaurant is in. Note that there are unicode in the names

City Group: Type of the city. Big cities, or Other.

Type: Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile

P1, P2 - P37: There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators.

Revenue: The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis
"""

train_df=pd.read_csv('/content/train.csv')
test_df=pd.read_csv('/content/test.csv')

train_df.head()

train_df.info()

train_df.shape

test_df.head()

test_df.info()

test_df.shape

"""train.csv - the training set. Use this dataset for training your model.

test.csv - the test set. To deter manual "guess" predictions, Kaggle has supplemented the test set with additional "ignored" data.

# Data Cleaning
"""

train_df.isnull().sum()

test_df.isnull().sum()

train_df.duplicated().sum()

test_df.duplicated().sum()

train_df.describe()

test_df.describe()

test_df.drop("Id", axis=1, inplace=True)

train_df["Open Date"] = pd.to_datetime(train_df["Open Date"])
train_df["Day"] = train_df["Open Date"].dt.day
train_df["Day_Name"] = train_df["Open Date"].dt.day_name()
train_df["Month"] = train_df["Open Date"].dt.month
train_df["Years"] = train_df["Open Date"].dt.year
train_df.drop("Open Date",axis=1,inplace=True)
#########
test_df["Open Date"] = pd.to_datetime(test_df["Open Date"])
test_df["Day"] = test_df["Open Date"].dt.day
test_df["Day_Name"] = test_df["Open Date"].dt.day_name()
test_df["Month"] = test_df["Open Date"].dt.month
test_df["Years"] = test_df["Open Date"].dt.year
test_df.drop("Open Date",axis=1,inplace=True)

train_df["City"].unique()

train_df["City Group"].unique()

"""# EDA"""

plt.figure(figsize=(12,7))
sns.countplot(y= train_df["City"],palette="summer")
plt.xlabel("Number of Restaurants")
plt.tight_layout()

city_df = train_df.groupby("City")["revenue"].agg(["mean","sum"]).reset_index()
fig , ax = plt.subplots(2,1,figsize = (20,15))
ax1 = sns.pointplot(x=city_df["City"],y=city_df["sum"],ax=ax[0])
ax1.tick_params(axis= "x" ,labelsize=15,labelrotation=75)
ax1.tick_params(axis= "y" ,labelsize=15)
ax1.set_xlabel("City", fontsize=15)
ax1.set_ylabel("Sum of Revenue", fontsize=15)
ax2 = sns.pointplot(x=city_df["City"],y=city_df["mean"],ax=ax[1],color = "r")
ax2.tick_params(axis= "x" ,labelsize=15,labelrotation=75)
ax2.tick_params(axis= "y" ,labelsize=15)
ax2.set_xlabel("City", fontsize=15)
ax2.set_ylabel("Mean of Revenue", fontsize=15)
plt.tight_layout()

plt.figure(figsize=(8,5))
group_dict = dict(train_df["City Group"].value_counts())
plt.pie(group_dict.values(),labels=group_dict.keys(),wedgeprops={"edgecolor":"black"},autopct="%1.1f%%")
plt.tight_layout()

cityg_df = train_df.groupby("City Group")["revenue"].agg(["mean","sum"]).reset_index()
fig , ax = plt.subplots(2,1,figsize = (18,12))
ax1 = sns.barplot(x=cityg_df["City Group"],y=cityg_df["sum"],ax=ax[0],palette="summer")
ax1.set_ylabel("Sum of Revenue")
ax2 = sns.barplot(x=cityg_df["City Group"],y=cityg_df["mean"],ax=ax[1],palette="summer")
ax2.set_ylabel("Mean of Revenue")
plt.tight_layout()

plt.figure(figsize=(8,5))
type_dict = dict(train_df["Type"].value_counts())
ex = [0,0.03,0.7]
plt.pie(type_dict.values(),labels=type_dict.keys(),explode = ex,wedgeprops={"edgecolor":"black"},autopct="%1.1f%%")
plt.tight_layout()

type_df = train_df.groupby("Type")["revenue"].agg(["mean","sum"]).reset_index()
fig , ax = plt.subplots(2,1,figsize = (18,12))
ax1 = sns.barplot(x=type_df["Type"],y=type_df["sum"],ax=ax[0],palette="summer")
ax1.set_ylabel("Sum of Revenue")
ax2 = sns.barplot(x=type_df["Type"],y=type_df["mean"],ax=ax[1],palette="summer")
ax2.set_ylabel("Mean of Revenue")
plt.show()

Month_df = train_df.groupby("Month")["revenue"].agg(["mean","sum"]).reset_index()
fig , ax = plt.subplots(2,1,figsize = (18,12))
ax1 = sns.barplot(x=Month_df["Month"],y=Month_df["sum"],ax=ax[0],palette = "summer")
ax1.set_ylabel("Sum of Revenue")
ax2 = sns.barplot(x=Month_df["Month"],y=Month_df["mean"],ax=ax[1],palette = "summer")
ax2.set_ylabel("Mean of Revenue")
plt.tight_layout()

Years_df = train_df.groupby("Years")["revenue"].agg(["mean","sum"]).reset_index()
fig , ax = plt.subplots(2,1,figsize = (18,12))
ax1 = sns.barplot(x=Years_df["Years"],y=Years_df["sum"],ax=ax[0],palette = "summer")
ax1.set_ylabel("Sum of Revenue")
ax2 = sns.barplot(x=Years_df["Years"],y=Years_df["mean"],ax=ax[1],palette = "summer")
ax2.set_ylabel("Mean of Revenue")
plt.tight_layout()

plt.figure(figsize=(12,7))
sns.distplot(x= train_df["revenue"])
plt.tight_layout()

"""# Encoding"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

column_to_encode = 'City'

# Tạo instance của LabelEncoder
encoder = LabelEncoder()
# Fit và transform dữ liệu
train_df[column_to_encode] = encoder.fit_transform(train_df[column_to_encode])
# Kiểm tra kết quả
print(train_df[column_to_encode].head())

train_df['City Group']=train_df['City Group'].map({'Big Cities':1,'Other':0})

train_df['Type']=train_df['Type'].map({'FC':0,'IL':1,'DT':2,'MB':3})

train_df.drop('Day_Name',axis=1,inplace=True)

train_df.head()

"""# Correlation"""

plt.figure(figsize=(12,7))
sns.heatmap(train_df.corr())
plt.tight_layout()

corr = train_df.corr()["revenue"].abs()
sorted_corr = corr.sort_values()
num_to_drop = int(0.75* len(train_df.columns))
cols_to_drop = sorted_corr.iloc[:num_to_drop].index
df_corr = train_df.drop(cols_to_drop,axis=1)

plt.figure(figsize=(15,10))
sns.heatmap(df_corr.corr(),annot=True)
plt.tight_layout()

"""# Models"""

revenue_mean = train_df['revenue'].mean()
revenue_mean

import matplotlib.pyplot as plt

# Biểu đồ phân bố dữ liệu revenue trước khi Log Scaling
plt.hist(train_df['revenue'])
plt.xlabel('Revenue')
plt.ylabel('Frequency')
plt.title('Distribution of Revenue before Log Scaling')
plt.show()

# Thực hiện Log Scaling
train_df['revenue_log_scaled'] = np.log1p(train_df['revenue'])

# Biểu đồ phân bố dữ liệu revenue sau khi Log Scaling
plt.hist(train_df['revenue_log_scaled'])
plt.xlabel('Log-Scaled Revenue')
plt.ylabel('Frequency')
plt.title('Distribution of Revenue after Log Scaling')
plt.show()

# Lựa chọn cột cần chuẩn hóa log scaling
column_to_scale = 'revenue'
# Thêm cột mới lưu trữ giá trị log scaling
train_df['revenue_log_scaled'] = np.log1p(train_df[column_to_scale])

X = train_df.drop("revenue_log_scaled",axis=1)
y = train_df["revenue_log_scaled"]
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)

"""**Linear Regression**"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
# Khởi tạo mô hình
model = LinearRegression()
model.fit(X_train, y_train)
# Dự đoán giá trị trên tập test
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print('Mean Squared Error (MSE):', mse)
print('Root Mean Squared Error (RMSE):', rmse)
mae = mean_absolute_error(y_test, y_pred)
print('Mean Absolute Error (MAE):', mae)
mape = np.mean(np.abs((y_pred - y_test) / y_test)) * 100
print('Mean Absolute Percentage Error (MAPE):', mape)

"""**Support Vector Regressor**"""

from sklearn.svm import SVR
model = SVR()

# Huấn luyện mô hình trên tập train
model.fit(X_train, y_train)
# Dự đoán giá trị trên tập test
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print('Mean Squared Error (MSE):', mse)
print('Root Mean Squared Error (RMSE):', rmse)
mae = mean_absolute_error(y_test, y_pred)
print('Mean Absolute Error (MAE):', mae)
mape = np.mean(np.abs((y_pred - y_test) / y_test)) * 100
print('Mean Absolute Percentage Error (MAPE):', mape)

"""Xgboost"""

import pandas as pd
from sklearn.model_selection import train_test_split
import xgboost as xgb

model = xgb.XGBRegressor()
model.fit(X_train, y_train)

# Predict values on the test data
y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print('Mean Squared Error (MSE):', mse)
print('Root Mean Squared Error (RMSE):', rmse)
mae = mean_absolute_error(y_test, y_pred)
print('Mean Absolute Error (MAE):', mae)
mape = np.mean(np.abs((y_pred - y_test) / y_test)) * 100
print('Mean Absolute Percentage Error (MAPE):', mape)

"""**Lightgbm**"""

import lightgbm as lgb
model = lgb.LGBMRegressor(verbose=-100)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

# Print the evaluation metrics
print('Mean Squared Error (MSE):', mse)
print('Root Mean Squared Error (RMSE):', rmse)
print('Mean Absolute Error (MAE):', mae)
print('Mean Absolute Percentage Error (MAPE):', mape)